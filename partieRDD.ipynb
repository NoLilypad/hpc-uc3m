{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89d14e19-b330-4599-aab5-7a0f2aebd440",
   "metadata": {},
   "source": [
    "# Introduction "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ece27a7-2ec7-42f6-8d17-f0e0b11b4393",
   "metadata": {},
   "source": [
    "This part demonstrates the use of Apache Spark RDDs (Resilient Distributed Datasets) for analyzing NYC taxi trip data from 2023. RDD are a low level Spark API, offering fine-grained control over distributed data processing through map-reduce operations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b468eb51-0679-4c12-b894-e304c79e3484",
   "metadata": {},
   "source": [
    "# 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf08925d-8c59-42b7-a090-18ecba1b94ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Version: 4.0.1\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "from operator import add # Useful for some RDD operations like reduce\n",
    "import time # Necessary to measure execution time\n",
    "\n",
    "# Initialization of the SparkSession and SparkContext\n",
    "# The two lines of memory config to avoid kernel crash\n",
    "spark = SparkSession.builder \\\n",
    "    .master('local[*]') \\\n",
    "    .appName(\"Analyse Taxis NYC RDD Simple v3 - 1 Month\") \\\n",
    "    .config(\"spark.driver.memory\", \"6g\") \\\n",
    "    .config(\"spark.executor.memory\", \"8g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "\n",
    "# Display configuration information\n",
    "print(f\"Spark Version: {spark.version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "565ad523-840d-4109-8a70-6583c726c3c1",
   "metadata": {},
   "source": [
    "## Launch Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "88625768-6453-481f-b945-4951a0f37e79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.13/site-packages (7.1.1)\n",
      "Your runtime has 8.2 gigabytes of available RAM\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!pip install psutil\n",
    "from psutil import virtual_memory\n",
    "ram_gb = virtual_memory().total / 1e9\n",
    "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ee49c6d-7d0c-4db9-9442-a3f1cabb1cb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sparkContext.defaultParallelism"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf529f64-fa2f-4a80-8859-7252a326c2b6",
   "metadata": {},
   "source": [
    "# 2. Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ed9734d-52c9-4951-bbe6-4bd4a880f8a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading months of data from: taxi/yellow_tripdata_2023-*.parquet\n",
      "Data loading in DataFrame completed (11 months).\n",
      "\n",
      "Total number of records (Action .count() recommended only for testing on 1 month or after a filter): 35,243,460\n",
      "\n",
      "Data Schema:\n",
      "root\n",
      " |-- VendorID: integer (nullable = true)\n",
      " |-- tpep_pickup_datetime: timestamp_ntz (nullable = true)\n",
      " |-- tpep_dropoff_datetime: timestamp_ntz (nullable = true)\n",
      " |-- passenger_count: long (nullable = true)\n",
      " |-- trip_distance: double (nullable = true)\n",
      " |-- RatecodeID: long (nullable = true)\n",
      " |-- store_and_fwd_flag: string (nullable = true)\n",
      " |-- PULocationID: integer (nullable = true)\n",
      " |-- DOLocationID: integer (nullable = true)\n",
      " |-- payment_type: long (nullable = true)\n",
      " |-- fare_amount: double (nullable = true)\n",
      " |-- extra: double (nullable = true)\n",
      " |-- mta_tax: double (nullable = true)\n",
      " |-- tip_amount: double (nullable = true)\n",
      " |-- tolls_amount: double (nullable = true)\n",
      " |-- improvement_surcharge: double (nullable = true)\n",
      " |-- total_amount: double (nullable = true)\n",
      " |-- congestion_surcharge: double (nullable = true)\n",
      " |-- Airport_fee: double (nullable = true)\n",
      "\n",
      "\n",
      "DataFrame converted to RDD.\n",
      "RDD cached. Number of partitions: 17\n"
     ]
    }
   ],
   "source": [
    "# Path to load all Parquet files from 2023\n",
    "# Warning, do not put the January file yellow_tripdata_2023-01.parquet in the taxi folder because it has a different schema than the others.\n",
    "parquet_path_all = \"taxi/yellow_tripdata_2023-*.parquet\"\n",
    "\n",
    "print(f\"\\nLoading months of data from: {parquet_path_all}\")\n",
    "\n",
    "try:\n",
    "    # 1. Load as DataFrame (Spark reads Parquet optimally)\n",
    "    df_taxi = spark.read.parquet(parquet_path_all)\n",
    "    print(\"Data loading in DataFrame completed (11 months).\")\n",
    "\n",
    "    # 2. Display schema and preview for confirmation\n",
    "    print(f\"\\nTotal number of records (Action .count() recommended only for testing on 1 month or after a filter): {df_taxi.count():,}\")\n",
    "    print(\"\\nData Schema:\")\n",
    "    df_taxi.printSchema()\n",
    "\n",
    "    # 3. Convert to RDD for the RR analysis\n",
    "    taxi_rdd = df_taxi.rdd\n",
    "    print(\"\\nDataFrame converted to RDD.\")\n",
    "\n",
    "    # 4. Cache (to speed up repeated RDD actions)\n",
    "    taxi_rdd.cache()\n",
    "    print(f\"RDD cached. Number of partitions: {taxi_rdd.getNumPartitions()}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\nERROR: Problem loading data from 12 months.\")\n",
    "    print(e)\n",
    "    # Stop Spark in case of a loading error\n",
    "    spark.stop()\n",
    "    raise SystemExit(\"Shutdown due to data loading error.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72158caa-b2a9-44b6-adf9-3c6b14fbc641",
   "metadata": {},
   "source": [
    "# 3. Simple Analysis with RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ffb2c4c5-4fc4-4e65-8c6b-2f15f01834ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- RDD Study: Total number of trips ---\n",
      "Total number of trips in the sample (1 month): 35,243,460\n",
      "Computation time: 70.77 seconds\n"
     ]
    }
   ],
   "source": [
    "# This first very simple analysis allows us to check that the data is fully loaded and the spark context works as expected.\n",
    "if 'taxi_rdd' in locals(): # Check if RDD has been created\n",
    "\n",
    "    # ### Study 1: Count total number of trips (Action: count)\n",
    "    print(\"\\n--- RDD Study: Total number of trips ---\")\n",
    "    start_time = time.time()\n",
    "    try:\n",
    "        # The .count() action forces reading and returns the total number of elements.\n",
    "        total_trajets = taxi_rdd.count()\n",
    "        end_time = time.time()\n",
    "        print(f\"Total number of trips in the sample (1 month): {total_trajets:,}\")\n",
    "        print(f\"Computation time: {end_time - start_time:.2f} seconds\")\n",
    "    except Exception as e:\n",
    "        print(f\"Study 1 failed (RDD.count): {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "059d7976-ad07-448a-a746-c82a6ac9a768",
   "metadata": {},
   "source": [
    "# 4. Analysis of Pickup Zones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ac19ba8a-8b8a-4060-bfeb-eadefacac2b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- RDD Study: Top 10 pickup zones (PULocationID) ---\n",
      "Computation time: 36.32 seconds\n",
      "\n",
      "Top 10 pickup zones:\n",
      "| Zone ID | Number of Trips |\n",
      "|---|---|\n",
      "\n",
      "|   132   |    1,832,274    |\n",
      "|   237   |    1,643,721    |\n",
      "|   161   |    1,630,624    |\n",
      "|   236   |    1,458,193    |\n",
      "|   162   |    1,248,419    |\n",
      "|   138   |    1,216,071    |\n",
      "|   186   |    1,195,886    |\n",
      "|   230   |    1,171,690    |\n",
      "|   142   |    1,155,796    |\n",
      "|   170   |    1,043,327    |\n"
     ]
    }
   ],
   "source": [
    "# We can now proceed to a more complex analysis, and use the map reduce model we learned, to count and order pickup locations. \n",
    "if 'taxi_rdd' in locals(): # Ensure that the RDD is available\n",
    "\n",
    "    # --- RDD Study: Top 10 pickup zones (PULocationID) ---\n",
    "    print(\"\\n--- RDD Study: Top 10 pickup zones (PULocationID) ---\")\n",
    "    start_time_rdd = time.time()\n",
    "\n",
    "    try:\n",
    "        # 1. Map: (PULocationID, 1) - Extract the ID and assign a counter of 1\n",
    "        # Columns are accessible by name in the Row RDD.\n",
    "        rdd_counts = taxi_rdd.map(lambda row: (row['PULocationID'], 1))\n",
    "\n",
    "        # 2. ReduceByKey: Aggregate the number of trips by PULocationID (sum the '1's)\n",
    "        rdd_reduced = rdd_counts.reduceByKey(add)\n",
    "\n",
    "        # 3. Action (TakeOrdered): Get the 10 results with the highest value (count).\n",
    "        # First sort by count (element[1]) in descending order (negative).\n",
    "        # The result is a Python list [ (PULocationID, count), ... ]\n",
    "        top_10_locations = rdd_reduced.takeOrdered(10, key=lambda x: -x[1])\n",
    "\n",
    "        end_time_rdd = time.time()\n",
    "        time_rdd = end_time_rdd - start_time_rdd\n",
    "\n",
    "        # Display results\n",
    "        print(f\"Computation time: {time_rdd:.2f} seconds\")\n",
    "        print(\"\\nTop 10 pickup zones:\")\n",
    "        print(\"| Zone ID | Number of Trips |\")\n",
    "        print(\"|---|---|\\n\")\n",
    "        for location_id, count in top_10_locations:\n",
    "            print(f\"| {location_id:^7} | {count:^15,} |\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Study 2 failed (RDD.map/reduceByKey): {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d6da3ad-d6cf-4df6-903e-ff8a51e72e14",
   "metadata": {},
   "source": [
    "# 5. Analysis of Average Speed by Hour of Day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "96fe7e28-9267-4c52-8558-1dbc4009f86f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- RDD Study: Average speed by hour of day ---\n",
      "Computation time: 46.37 seconds\n",
      "\n",
      "Average Speed (mph) by Hour of Day:\n",
      "| Hour | Average Speed (mph) |\n",
      "|---|---|\n",
      "\n",
      "|   0   |        15.36        |\n",
      "|   1   |        14.63        |\n",
      "|   2   |        19.74        |\n",
      "|   3   |        20.71        |\n",
      "|   4   |        41.38        |\n",
      "|   5   |        41.32        |\n",
      "|   6   |        32.70        |\n",
      "|   7   |        22.63        |\n",
      "|   8   |        15.67        |\n",
      "|   9   |        15.03        |\n",
      "|  10   |        12.16        |\n",
      "|  11   |        12.15        |\n",
      "|  12   |        11.95        |\n",
      "|  13   |        12.63        |\n",
      "|  14   |        12.07        |\n",
      "|  15   |        12.90        |\n",
      "|  16   |        11.03        |\n",
      "|  17   |        12.01        |\n",
      "|  18   |        11.94        |\n",
      "|  19   |        13.74        |\n",
      "|  20   |        13.53        |\n",
      "|  21   |        14.65        |\n",
      "|  22   |        15.12        |\n",
      "|  23   |        15.45        |\n"
     ]
    }
   ],
   "source": [
    "from datetime import timedelta # Necessary for time calculations\n",
    "\n",
    "if 'taxi_rdd' in locals(): # Ensure that the RDD is available\n",
    "\n",
    "    \n",
    "    print(\"\\n--- RDD Study: Average speed by hour of day ---\")\n",
    "    start_time_rdd_complex = time.time()\n",
    "\n",
    "    try:\n",
    "        # A helper function to extract hour and calculate duration in seconds\n",
    "        def extract_hour_and_duration(row):\n",
    "            pickup_time = row['tpep_pickup_datetime']\n",
    "            dropoff_time = row['tpep_dropoff_datetime']\n",
    "\n",
    "            # Ensure that values are not null and dropoff is after pickup\n",
    "            if pickup_time and dropoff_time and dropoff_time > pickup_time:\n",
    "                trip_duration_seconds = (dropoff_time - pickup_time).total_seconds()\n",
    "                # Filter unrealistic durations (e.g., more than 24h) and null durations\n",
    "                if trip_duration_seconds > 60 and trip_duration_seconds < (24 * 3600):\n",
    "                    # Key: Hour of pickup (0-23)\n",
    "                    hour = pickup_time.hour\n",
    "                    # Values: (Distance, Duration in hours)\n",
    "                    trip_distance = row['trip_distance']\n",
    "                    trip_duration_hours = trip_duration_seconds / 3600.0\n",
    "                    return (hour, (trip_distance, trip_duration_hours))\n",
    "            return None # Ignore invalid rows\n",
    "\n",
    "        # 1 Map: (Hour, (Distance, Duration in hours)) + Filtering\n",
    "        rdd_distance_duration = taxi_rdd.map(extract_hour_and_duration).filter(lambda x: x is not None)\n",
    "\n",
    "        # 2 ReduceByKey: (Hour, (Total Distance, Total Duration))\n",
    "        # Aggregation function: (dist1, dur1) + (dist2, dur2) = (dist1+dist2, dur1+dur2)\n",
    "        rdd_totals = rdd_distance_duration.reduceByKey(lambda a, b: (a[0] + b[0], a[1] + b[1]))\n",
    "\n",
    "        # 3 Map: (Hour, Average Speed) where Speed = TotalDistance / TotalDuration\n",
    "        rdd_avg_speed = rdd_totals.map(lambda x: (x[0], x[1][0] / x[1][1])) # x[0] = Hour, x[1][0] = Total Dist, x[1][1] = Total Duration\n",
    "\n",
    "        # 4 Action (Collect) and Sort: Retrieve result and sort by hour\n",
    "        avg_speed_by_hour = sorted(rdd_avg_speed.collect(), key=lambda x: x[0])\n",
    "\n",
    "        end_time_rdd_complex = time.time()\n",
    "        time_rdd_complex = end_time_rdd_complex - start_time_rdd_complex\n",
    "\n",
    "        # We display results\n",
    "        print(f\"Computation time: {time_rdd_complex:.2f} seconds\")\n",
    "        print(\"\\nAverage Speed (mph) by Hour of Day:\")\n",
    "        print(\"| Hour | Average Speed (mph) |\")\n",
    "        print(\"|---|---|\\n\")\n",
    "        for hour, avg_speed in avg_speed_by_hour:\n",
    "            print(f\"| {hour:^5} | {avg_speed:^19.2f} |\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Complex RDD Study failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfeeaf1c-48d9-47ca-af36-123f6da6f401",
   "metadata": {},
   "source": [
    "# Conclusion "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db4e1322-924a-4280-b5a2-977e268b3c68",
   "metadata": {},
   "source": [
    "The average speed analysis by hour of day reveals coherent and expected patterns in NYC taxi traffic. The results show higher average speeds during late-night and early-morning hours (approximately midnight to 5 AM) when the city is quieter with less traffic congestion. Conversely, lower average speeds are observed during typical rush hours and busy daytime periods when traffic density increases. This pattern aligns with real-world expectations: taxis can travel faster on less congested streets during off-peak hours. The RDD-based implementation successfully processed millions of trip records, using the map-reduce paradigm we saw in class."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
